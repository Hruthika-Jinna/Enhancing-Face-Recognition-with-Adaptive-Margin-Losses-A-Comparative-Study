{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3-hP0mwPmY2"
      },
      "source": [
        "Import necessary libraries\n",
        "and Mount Google Drive to access data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "hGBfDYDKPdib",
        "outputId": "345b15bb-4ec4-4bd5-c0f5-25df893aa7c7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-36b9f4dfc883>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Import the required libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Import the required libraries\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data.dataset import Subset\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import zipfile\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.layers import Layer, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.applications import ResNet50V2\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras import regularizers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKgpsDAOP02P"
      },
      "source": [
        "Extract data which is in zip file from google drive and store in data folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixfL3bSOPvyP"
      },
      "outputs": [],
      "source": [
        "# Extract zip file\n",
        "# Install the gdown library if not already installed\n",
        "!pip install gdown\n",
        "\n",
        "# Import required libraries\n",
        "import os\n",
        "import zipfile\n",
        "import gdown\n",
        "\n",
        "# Google Drive file ID from the sharing link\n",
        "file_id = '1gwIcChfW5Zbe-Qw0Rqi2vOVWyQc2PAY1'\n",
        "\n",
        "# URL to download the file from Google Drive\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "# Target directory for extraction\n",
        "target_dir = '/content/data'  # Replace with your desired target directory\n",
        "\n",
        "# Create the target directory if it doesn't exist\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Get the file name from the URL\n",
        "file_name = os.path.basename(url)\n",
        "\n",
        "# Construct the file path\n",
        "zip_file_path = os.path.join(target_dir, file_name)\n",
        "\n",
        "# Download the file using gdown\n",
        "gdown.download(url, zip_file_path, quiet=False)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(target_dir)\n",
        "\n",
        "# Delete the downloaded zip file\n",
        "os.remove(zip_file_path)\n",
        "\n",
        "print('File extracted successfully.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK8pPekOQRU2"
      },
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtmJyPemQAHu"
      },
      "outputs": [],
      "source": [
        "# Define transformations for data preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((56, 56)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbd0xvdXQXQ-"
      },
      "source": [
        "Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61i7kadpQT31"
      },
      "outputs": [],
      "source": [
        "# Load dataset from extracted folder\n",
        "data_path = '/content/data/lfw-deepfunneled'  # Replace with the extracted data path\n",
        "dataset = ImageFolder(root=data_path, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpAnYZXsQjwH"
      },
      "source": [
        "**L2** **Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybjtwkWHQcVv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the l2_norm function\n",
        "def l2_norm(input, axis=1):\n",
        "    norm = torch.norm(input, 2, axis, keepdim=True)\n",
        "    output = torch.div(input, norm)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrOjv679Y5wJ"
      },
      "source": [
        "**SphereFace loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUNz-D9_Y4zP"
      },
      "outputs": [],
      "source": [
        "# Define the SphereFace head\n",
        "class SphereFace(nn.Module):\n",
        "    def __init__(self, embedding_size=512, classnum=2, m=4, s=64.0):\n",
        "        super(SphereFace, self).__init__()\n",
        "        self.classnum = classnum\n",
        "        self.kernel = nn.Parameter(torch.Tensor(embedding_size, classnum))\n",
        "\n",
        "        # Initialize the kernel\n",
        "        self.kernel.data.uniform_(-1, 1).renorm_(2, 1, 1e-5).mul_(1e5)\n",
        "        self.m = m\n",
        "        self.s = s\n",
        "\n",
        "    def forward(self, embeddings, labels):\n",
        "        kernel_norm = l2_norm(self.kernel, axis=0)\n",
        "        cosine = torch.mm(embeddings, kernel_norm)\n",
        "\n",
        "        index_mask = torch.zeros_like(cosine)\n",
        "        index_mask.scatter_(1, labels.view(-1, 1), 1)\n",
        "\n",
        "        theta = torch.acos(torch.clamp(cosine, -1 + 1e-7, 1 - 1e-7))\n",
        "        sphereface_logits = self.s * (torch.cos(self.m * theta) - cosine) * index_mask + cosine\n",
        "\n",
        "        return sphereface_logits\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8V8PgSeY-Lu"
      },
      "source": [
        "**CosFace loss Function**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeY5GgNuYz9W"
      },
      "outputs": [],
      "source": [
        "# Define the CosFace head\n",
        "class CosFace(nn.Module):\n",
        "    def __init__(self, embedding_size=512, classnum=2, m=0.35, s=64.0):\n",
        "        super(CosFace, self).__init__()\n",
        "        self.classnum = classnum\n",
        "        self.kernel = nn.Parameter(torch.Tensor(embedding_size, classnum))\n",
        "\n",
        "        # Initialize the kernel\n",
        "        self.kernel.data.uniform_(-1, 1).renorm_(2, 1, 1e-5).mul_(1e5)\n",
        "        self.m = m\n",
        "        self.s = s\n",
        "\n",
        "    def forward(self, embeddings, labels):\n",
        "        kernel_norm = l2_norm(self.kernel, axis=0)\n",
        "        cosine = torch.mm(embeddings, kernel_norm)\n",
        "\n",
        "        index_mask = torch.zeros_like(cosine)\n",
        "        index_mask.scatter_(1, labels.view(-1, 1), 1)\n",
        "\n",
        "        cosface_logits = cosine - self.m * index_mask\n",
        "        cosface_logits *= self.s\n",
        "\n",
        "        return cosface_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvYuelfjQnjS"
      },
      "source": [
        "**ArcFace Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yf6qg2TQkdx"
      },
      "outputs": [],
      "source": [
        "# Define the ArcFace head\n",
        "class ArcFace(nn.Module):\n",
        "    def __init__(self, embedding_size=512, classnum=2, m=0.5, s=64.0):\n",
        "        super(ArcFace, self).__init__()\n",
        "        self.classnum = classnum\n",
        "        self.kernel = nn.Parameter(torch.Tensor(embedding_size, classnum))\n",
        "\n",
        "        # Initial kernel\n",
        "        self.kernel.data.uniform_(-1, 1).renorm_(2, 1, 1e-5).mul_(1e5)\n",
        "        self.m = m\n",
        "        self.eps = 1e-3\n",
        "        self.s = s\n",
        "\n",
        "    def forward(self, embeddings, labels):\n",
        "        kernel_norm = l2_norm(self.kernel, axis=0)\n",
        "        cosine = torch.mm(embeddings, kernel_norm)\n",
        "        cosine = cosine.clamp(-1 + self.eps, 1 - self.eps)  # For stability\n",
        "\n",
        "        index_mask = torch.zeros_like(cosine)\n",
        "        index_mask.scatter_(1, labels.view(-1, 1), 1)\n",
        "\n",
        "        theta = torch.acos(cosine)\n",
        "        margin_theta = theta + self.m\n",
        "        margin_cosine = torch.cos(margin_theta)\n",
        "\n",
        "        arcface_logits = self.s * (margin_cosine - cosine) * index_mask + cosine\n",
        "\n",
        "        return arcface_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQn-g9rTQ3Fe"
      },
      "source": [
        "**AdaFace Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFDnMnemQxou"
      },
      "outputs": [],
      "source": [
        "# Define the AdaFace head\n",
        "class AdaFace(nn.Module):\n",
        "    def __init__(self, embedding_size=512, classnum=2, m=0.4, h=0.333, s=64., t_alpha=1.0):\n",
        "        super(AdaFace, self).__init__()\n",
        "        self.classnum = classnum\n",
        "        self.kernel = nn.Parameter(torch.Tensor(embedding_size, classnum))\n",
        "\n",
        "        # Initial kernel\n",
        "        self.kernel.data.uniform_(-1, 1).renorm_(2, 1, 1e-5).mul_(1e5)\n",
        "        self.m = m\n",
        "        self.eps = 1e-3\n",
        "        self.h = h\n",
        "        self.s = s\n",
        "\n",
        "        # EMA prep\n",
        "        self.t_alpha = t_alpha\n",
        "        self.register_buffer('t', torch.zeros(1))\n",
        "        self.register_buffer('batch_mean', torch.ones(1) * (20))\n",
        "        self.register_buffer('batch_std', torch.ones(1) * 100)\n",
        "\n",
        "        print('\\nAdaFace with the following property')\n",
        "        print('self.m', self.m)\n",
        "        print('self.h', self.h)\n",
        "        print('self.s', self.s)\n",
        "        print('self.t_alpha', self.t_alpha)\n",
        "\n",
        "    def forward(self, embeddings, norms, label):\n",
        "        kernel_norm = l2_norm(self.kernel, axis=0)\n",
        "        cosine = torch.mm(embeddings, kernel_norm)\n",
        "        cosine = cosine.clamp(-1 + self.eps, 1 - self.eps)  # For stability\n",
        "\n",
        "        safe_norms = torch.clip(norms, min=0.001, max=100)  # For stability\n",
        "        safe_norms = safe_norms.clone().detach()\n",
        "\n",
        "        # Update batchmean batchstd\n",
        "        with torch.no_grad():\n",
        "            mean = safe_norms.mean().detach()\n",
        "            std = safe_norms.std().detach()\n",
        "            self.batch_mean = mean * self.t_alpha + (1 - self.t_alpha) * self.batch_mean\n",
        "            self.batch_std = std * self.t_alpha + (1 - self.t_alpha) * self.batch_std\n",
        "\n",
        "        margin_scaler = (safe_norms - self.batch_mean) / (self.batch_std + self.eps)  # 66% between -1, 1\n",
        "        margin_scaler = margin_scaler * self.h  # 68% between -0.333 ,0.333 when h:0.333\n",
        "        margin_scaler = torch.clip(margin_scaler, -1, 1)\n",
        "\n",
        "        # G_angular\n",
        "        m_arc = torch.zeros(label.size()[0], cosine.size()[1], device=cosine.device)\n",
        "        m_arc.scatter_(1, label.reshape(-1, 1), 1.0)\n",
        "        g_angular = self.m * margin_scaler * -1\n",
        "        m_arc = m_arc * g_angular\n",
        "        theta = cosine.acos()\n",
        "        theta_m = torch.clip(theta + m_arc, min=self.eps, max=math.pi - self.eps)\n",
        "        cosine = theta_m.cos()\n",
        "\n",
        "        # G_additive\n",
        "        m_cos = torch.zeros(label.size()[0], cosine.size()[1], device=cosine.device)\n",
        "        m_cos.scatter_(1, label.reshape(-1, 1), 1.0)\n",
        "        g_add = self.m + (self.m * margin_scaler)\n",
        "        m_cos = m_cos * g_add\n",
        "        cosine = cosine - m_cos\n",
        "\n",
        "        # Scale\n",
        "        scaled_cosine_m = cosine * self.s\n",
        "        return scaled_cosine_m\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02W0iZvvQ-sn"
      },
      "source": [
        "**CNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3C_EywPQ690"
      },
      "outputs": [],
      "source": [
        "# Define the CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, embedding_size=512, num_classes=2):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.fc = nn.Linear(64 * 14 * 14, embedding_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kA6A2hORdVo"
      },
      "outputs": [],
      "source": [
        "# Define the training loop function\n",
        "def train_model(model, optimizer, dataloader, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate norms and normalized embedding\n",
        "        norms = torch.norm(outputs, 2, dim=1, keepdim=True)\n",
        "        normalized_embedding = outputs / norms\n",
        "\n",
        "        # Calculate SphereFace, CosFace, or AdaFace logits\n",
        "        if isinstance(criterion, ArcFace):\n",
        "            logits = criterion(normalized_embedding, labels)\n",
        "        elif isinstance(criterion, AdaFace):\n",
        "            logits = criterion(normalized_embedding, norms, labels)\n",
        "        elif isinstance(criterion, CosFace):\n",
        "            logits = criterion(normalized_embedding, labels)\n",
        "        elif isinstance(criterion, SphereFace):\n",
        "            logits = criterion(normalized_embedding, labels)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid criterion\")\n",
        "\n",
        "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = logits.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    accuracy = 100.0 * correct / total\n",
        "\n",
        "    return average_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djg5kUxrR0PY"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "# Split dataset into train, validation, and test sets\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, remaining_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
        "val_dataset, test_dataset = random_split(remaining_dataset, [val_size, test_size])\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cN7qRO7R9wb"
      },
      "outputs": [],
      "source": [
        "# Get the actual number of classes from the dataset\n",
        "classnum = len(dataset.classes)\n",
        "print(classnum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOGSJlh9SXbG"
      },
      "outputs": [],
      "source": [
        "embedding_size = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BytaohpsSC6u"
      },
      "outputs": [],
      "source": [
        "# Instantiate CosFace,SphereFace,ArcFace and AdaFace\n",
        "sphereface = SphereFace(embedding_size=embedding_size, classnum=classnum, m=4, s=64.0)\n",
        "cosface = CosFace(embedding_size=embedding_size, classnum=classnum, m=0.35, s=64.0)\n",
        "arcface = ArcFace(embedding_size=embedding_size, classnum=classnum, m=0.5, s=64.0)\n",
        "adaface = AdaFace(embedding_size=embedding_size, classnum=classnum, m=0.4, h=0.333, s=64., t_alpha=0.01)\n",
        "\n",
        "# Instantiate the CNN model and optimizer\n",
        "model = SimpleCNN(embedding_size=embedding_size, num_classes=classnum)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut5W-E_hSHAY"
      },
      "outputs": [],
      "source": [
        "# Move models and optimizer to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "sphereface.to(device)\n",
        "cosface.to(device)\n",
        "arcface.to(device)\n",
        "adaface.to(device)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vV25FnRjSJeA"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "num_epochs = 40\n",
        "for epoch in range(num_epochs):\n",
        "    arcface_loss, arcface_accuracy = train_model(model, optimizer, train_loader, arcface, device)\n",
        "    adaface_loss, adaface_accuracy = train_model(model, optimizer, train_loader, adaface, device)\n",
        "    cosface_loss, cosface_accuracy = train_model(model, optimizer, train_loader, cosface, device)\n",
        "    sphereface_loss, sphereface_accuracy = train_model(model, optimizer, train_loader, sphereface, device)\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}]')\n",
        "\n",
        "    #print(f'SphereFace Loss: {sphereface_loss:.4f} | SphereFace Accuracy: {sphereface_accuracy:.2f}%')\n",
        "    print(f'CosFace Loss: {cosface_loss:.4f} | CosFace Accuracy: {cosface_accuracy:.2f}%')\n",
        "    print(f'ArcFace Loss: {arcface_loss:.4f} | ArcFace Accuracy: {arcface_accuracy:.2f}%')\n",
        "    print(f'AdaFace Loss: {adaface_loss:.4f} | AdaFace Accuracy: {adaface_accuracy:.2f}%')\n",
        "\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Calculate norms and normalized embedding\n",
        "            norms = torch.norm(outputs, 2, dim=1, keepdim=True)\n",
        "            normalized_embedding = outputs / norms\n",
        "\n",
        "            # Calculate logits based on the criterion\n",
        "            if isinstance(criterion, ArcFace):\n",
        "                logits = criterion(normalized_embedding, labels)\n",
        "            elif isinstance(criterion, AdaFace):\n",
        "                logits = criterion(normalized_embedding, norms, labels)\n",
        "            elif isinstance(criterion, CosFace):\n",
        "                logits = criterion(normalized_embedding, labels)\n",
        "            elif isinstance(criterion, SphereFace):\n",
        "                logits = criterion(normalized_embedding, labels)\n",
        "            else:\n",
        "                raise ValueError(\"Invalid criterion\")\n",
        "\n",
        "            _, predicted = logits.max(1)\n",
        "            total_correct += predicted.eq(labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    accuracy = 100.0 * total_correct / total_samples\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "16uQRyJ9ptQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7kAlqj-SNBg"
      },
      "outputs": [],
      "source": [
        "# Evaluate on validation set\n",
        "sphereface_val_accuracy = evaluate_model(model, val_loader, sphereface, device)\n",
        "cosface_val_accuracy = evaluate_model(model, val_loader, cosface, device)\n",
        "arcface_val_accuracy = evaluate_model(model, val_loader, arcface, device)\n",
        "adaface_val_accuracy = evaluate_model(model, val_loader, adaface, device)\n",
        "#print(f'Validation Accuracy - SphereFace: {sphereface_val_accuracy:.2f}%')\n",
        "print(f'Validation Accuracy - CosFace: {cosface_val_accuracy:.2f}%')\n",
        "print(f'Validation Accuracy - ArcFace: {arcface_val_accuracy:.2f}%')\n",
        "print(f'Validation Accuracy - AdaFace: {adaface_val_accuracy:.2f}%')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6QOf3xZXbk2"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "sphereface_test_accuracy = evaluate_model(model, test_loader, sphereface, device)\n",
        "cosface_test_accuracy = evaluate_model(model, test_loader, cosface, device)\n",
        "arcface_test_accuracy = evaluate_model(model, test_loader, arcface, device)\n",
        "adaface_test_accuracy = evaluate_model(model, test_loader, adaface, device)\n",
        "\n",
        "#print(f'Test Accuracy - SphereFace: {sphereface_test_accuracy:.2f}%')\n",
        "print(f'Test Accuracy - CosFace: {cosface_test_accuracy:.2f}%')\n",
        "print(f'Test Accuracy - ArcFace: {arcface_test_accuracy:.2f}%')\n",
        "print(f'Test Accuracy - AdaFace: {adaface_test_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Import necessary libraries\n",
        "# from sklearn.model_selection import ParameterGrid\n",
        "# import copy\n",
        "\n",
        "# # Define the hyperparameter grid\n",
        "# hyperparameters = {\n",
        "#     'loss_function': ['AdaFace', 'CosFace',  'ArcFace'],\n",
        "#     'embedding_size': [128, 256, 512],\n",
        "#     'm_values': [0.3, 0.4, 0.5],\n",
        "#     's_values': [32.0, 64.0, 128.0],\n",
        "#     'h_values': [0.2, 0.333, 0.5],\n",
        "#     't_alpha_values': [0.01, 0.1, 0.5],\n",
        "#     'lr': [0.001, 0.0001]\n",
        "# }\n",
        "\n",
        "# # Create a list to store the results\n",
        "# results = []\n",
        "\n",
        "# # Iterate through the hyperparameter grid\n",
        "# for params in ParameterGrid(hyperparameters):\n",
        "#     loss_function = params['loss_function']\n",
        "#     embedding_size = params['embedding_size']\n",
        "\n",
        "#     # Instantiate the appropriate loss function\n",
        "#     if loss_function == 'AdaFace':\n",
        "#         loss_fn = AdaFace(embedding_size=embedding_size, classnum=classnum,\n",
        "#                           m=params['m_values'], h=params['h_values'],\n",
        "#                           s=params['s_values'], t_alpha=params['t_alpha_values'])\n",
        "#     elif loss_function == 'CosFace':\n",
        "#         loss_fn = CosFace(embedding_size=embedding_size, classnum=classnum,\n",
        "#                           m=params['m_values'], s=params['s_values'])\n",
        "#     elif loss_function == 'ArcFace':\n",
        "#         loss_fn = ArcFace(embedding_size=embedding_size, classnum=classnum,\n",
        "#                           m=params['m_values'], s=params['s_values'])\n",
        "#     else:\n",
        "#         raise ValueError(\"Invalid loss function\")\n",
        "\n",
        "#     # Create a new instance of the model for each set of hyperparameters\n",
        "#     model = SimpleCNN(embedding_size=embedding_size, num_classes=classnum)\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
        "\n",
        "#     # Move the model and loss function to the device\n",
        "#     model.to(device)\n",
        "#     loss_fn.to(device)\n",
        "\n",
        "#     # Training loop\n",
        "#     num_epochs = 10\n",
        "#     for epoch in range(num_epochs):\n",
        "#         train_loss, train_accuracy = train_model(model, optimizer, train_loader, loss_fn, device)\n",
        "\n",
        "#     # Evaluate on the validation set\n",
        "#     val_accuracy = evaluate_model(model, val_loader, loss_fn, device)\n",
        "\n",
        "#     # Store the results\n",
        "#     result = copy.deepcopy(params)\n",
        "#     result['val_accuracy'] = val_accuracy\n",
        "#     results.append(result)\n",
        "\n",
        "# # Print the results\n",
        "# for result in results:\n",
        "#     print(result)\n"
      ],
      "metadata": {
        "id": "yePfVyqTIXHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load and preprocess the dataset (modify the data_dir accordingly)\n",
        "data_dir = '/content/data/lfw-deepfunneled'\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for class_name in os.listdir(data_dir):\n",
        "    class_path = os.path.join(data_dir, class_name)\n",
        "    if os.path.isdir(class_path):\n",
        "        for image_name in os.listdir(class_path):\n",
        "            image_path = os.path.join(class_path, image_name)\n",
        "            image = tf.keras.utils.load_img(image_path, target_size=(224, 224))\n",
        "            X.append(np.array(image))\n",
        "            Y.append(class_name)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "Y_labelEnc = label_encoder.fit_transform(Y)\n",
        "Y_onehot = tf.keras.utils.to_categorical(Y_labelEnc, num_classes=len(label_encoder.classes_))\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_onehot, test_size=0.25, random_state=104, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Jf0qTMgY94hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50V2\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Define the AdaFace loss function\n",
        "class AdaFaceLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, m=0.4, h=0.333, s=64.0, t_alpha=1.0, **kwargs):\n",
        "        super(AdaFaceLoss, self).__init__(**kwargs)\n",
        "        self.m = m\n",
        "        self.h = h\n",
        "        self.s = s\n",
        "        self.t_alpha = t_alpha\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        # Remove L2 normalization step\n",
        "        logits = tf.matmul(y_pred, tf.transpose(y_pred))\n",
        "\n",
        "        norms = tf.norm(y_pred, axis=1)\n",
        "        batch_mean = tf.reduce_mean(norms)\n",
        "        batch_std = tf.math.reduce_std(norms)\n",
        "        margin_scaler = (norms - batch_mean) / (batch_std + 1e-6) * self.h\n",
        "        margin_scaler = tf.clip_by_value(margin_scaler, -1.0, 1.0)\n",
        "\n",
        "        theta = tf.acos(tf.clip_by_value(logits, -1.0 + 1e-6, 1.0 - 1e-6))\n",
        "        target_theta = theta + (-self.m * margin_scaler)\n",
        "        target_logits = self.s * tf.cos(target_theta)\n",
        "\n",
        "        logits = self.s * logits\n",
        "        logits_softmax = tf.nn.softmax(logits, axis=-1)\n",
        "        target_logits_softmax = tf.nn.softmax(target_logits, axis=-1)\n",
        "\n",
        "        adaface_loss = tf.keras.losses.CategoricalCrossentropy()(logits_softmax, target_logits_softmax)\n",
        "        return adaface_loss\n",
        "\n",
        "class ArcFaceLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, m=0.5, s=64.0, **kwargs):\n",
        "        super(ArcFaceLoss, self).__init__(**kwargs)\n",
        "        self.m = m\n",
        "        self.s = s\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        # Remove L2 normalization step\n",
        "        cosine_similarity = y_pred\n",
        "\n",
        "        theta = tf.acos(tf.clip_by_value(cosine_similarity, -1.0 + 1e-6, 1.0 - 1e-6))\n",
        "        target_theta = theta + self.m\n",
        "        target_logits = self.s * tf.cos(target_theta)\n",
        "\n",
        "        logits = self.s * cosine_similarity\n",
        "        logits_softmax = tf.nn.softmax(logits, axis=-1)\n",
        "        target_logits_softmax = tf.nn.softmax(target_logits, axis=-1)\n",
        "\n",
        "        arcface_loss = tf.keras.losses.CategoricalCrossentropy()(logits_softmax, target_logits_softmax)\n",
        "        return arcface_loss\n",
        "\n",
        "\n",
        "\n",
        "# Load and preprocess your data (X_train, Y_train, X_test, Y_test, label_encoder, etc.)\n",
        "\n",
        "# Define the full face recognition model using ResNet50V2\n",
        "base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "flatten_layer = Flatten()(base_model.output)\n",
        "dense_layer1 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(flatten_layer)\n",
        "dropout_layer1 = Dropout(0.5)(dense_layer1)\n",
        "dense_layer2 = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(dropout_layer1)\n",
        "dropout_layer2 = Dropout(0.5)(dense_layer2)\n",
        "output_layer = Dense(len(label_encoder.classes_), activation='softmax')(dropout_layer2)\n",
        "\n",
        "model = tf.keras.Model(inputs=base_model.input, outputs=output_layer)\n",
        "\n",
        "# Compile and train the model with the ArcFace loss function\n",
        "model.compile(loss=ArcFaceLoss(), optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(np.array(X_train), np.array(Y_train), batch_size=32, epochs=5, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model with ArcFace loss\n",
        "yhat = model.predict(np.array(X_test))\n",
        "yhat_indices = np.argmax(yhat, axis=1)\n",
        "yhat_labels = label_encoder.inverse_transform(yhat_indices)\n",
        "Y_test_indices = np.argmax(Y_test, axis=1)\n",
        "Y_test_labels = label_encoder.inverse_transform(Y_test_indices)\n",
        "acc_arcface = accuracy_score(Y_test_labels, yhat_labels)\n",
        "print('Accuracy with ArcFace: %.3f' % acc_arcface)\n",
        "\n",
        "# Compile and train the model with the AdaFace loss function\n",
        "model.compile(loss=AdaFaceLoss(), optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(np.array(X_train), np.array(Y_train), batch_size=32, epochs=5, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model with AdaFace loss\n",
        "yhat = model.predict(np.array(X_test))\n",
        "yhat_indices = np.argmax(yhat, axis=1)\n",
        "yhat_labels = label_encoder.inverse_transform(yhat_indices)\n",
        "Y_test_indices = np.argmax(Y_test, axis=1)\n",
        "Y_test_labels = label_encoder.inverse_transform(Y_test_indices)\n",
        "acc_adaface = accuracy_score(Y_test_labels, yhat_labels)\n",
        "print('Accuracy with AdaFace: %.3f' % acc_adaface)\n",
        "\n",
        "# Compare the performance of AdaFace and ArcFace\n",
        "print('Accuracy comparison:')\n",
        "print('AdaFace: %.3f' % acc_adaface)\n",
        "print('ArcFace: %.3f' % acc_arcface)\n",
        "\n",
        "# ... (rest of your code)\n",
        "\n"
      ],
      "metadata": {
        "id": "-2qWp1s399KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oRG8rUaq-EbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5hMmpNju9lUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jxZJryjjPTHg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}